{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model in order to compare with our best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from jsonargparse import CLI\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of data_set and vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")\n",
    "\n",
    "#####\n",
    "# training and test sets of transcription ids\n",
    "#####\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = []\n",
    "\n",
    "list_speaker = ['PM','ME','ID','UI']\n",
    "\n",
    "with open(\"training_labels.json\", \"r\") as file:\n",
    "    training_labels = json.load(file)\n",
    "\n",
    "X_train_text = {}   #dic of the text for every discussion for the train\n",
    "X_train_speaker = {}\n",
    "y_train = {}\n",
    "\n",
    "X_val_text = {}\n",
    "X_val_speaker = {}\n",
    "y_val = {}\n",
    "\n",
    "#all the val set which represent 1/5 of training_set\n",
    "val_set = ['ES2002a', 'ES2005b', 'ES2006c', 'ES2007d', 'ES2009a', 'ES2010b', 'ES2012c', 'ES2013d', 'ES2016a', 'IS1000b', 'IS1001c', 'IS1002d', 'IS1004a', 'IS1005b', 'IS1006c', 'IS1007d', 'TS3008a', 'TS3009b', 'TS3010c', 'TS3011d']\n",
    "train_set = []  #the other 4/5 of the training_set\n",
    "\n",
    "for transcription_id in training_set:\n",
    "    with open(path_to_training / f\"{transcription_id}.json\", \"r\") as file:\n",
    "        transcription = json.load(file)\n",
    "    X_train_text_transcription = []   #tab of the text for this discussion\n",
    "    X_train_speaker_transcription = []   #tab of one_hot_encending of speaker for this discussion\n",
    "    for utterance in transcription: \n",
    "        X_train_text_transcription.append(utterance[\"text\"])\n",
    "        total_text.append(utterance[\"text\"])\n",
    "        speaker_one_hot = [0,0,0,0]\n",
    "        speaker_one_hot[list_speaker.index(utterance[\"speaker\"])] = 1\n",
    "        X_train_speaker_transcription.append(speaker_one_hot)\n",
    "    if transcription_id in val_set:\n",
    "        y_val[transcription_id] = training_labels[transcription_id]\n",
    "        X_val_text[transcription_id] = X_train_text_transcription\n",
    "        X_val_speaker[transcription_id] = X_train_speaker_transcription\n",
    "    else:\n",
    "        train_set.append(transcription_id)\n",
    "        y_train[transcription_id] = training_labels[transcription_id]\n",
    "        X_train_text[transcription_id] = X_train_text_transcription\n",
    "        X_train_speaker[transcription_id] = X_train_speaker_transcription\n",
    "\n",
    "X_test_text = {}\n",
    "X_test_speaker = {}\n",
    "for transcription_id in test_set:\n",
    "    with open(path_to_test / f\"{transcription_id}.json\", \"r\") as file:\n",
    "        transcription = json.load(file)\n",
    "    X_test_text_transcription = []\n",
    "    X_test_speaker_transcription = []\n",
    "    for utterance in transcription:\n",
    "        X_test_text_transcription.append(utterance[\"text\"])\n",
    "        total_text.append(utterance[\"text\"])\n",
    "        speaker_one_hot = [0,0,0,0]\n",
    "        speaker_one_hot[list_speaker.index(utterance[\"speaker\"])] = 1\n",
    "        X_test_speaker_transcription.append(speaker_one_hot)\n",
    "    X_test_text[transcription_id] = X_test_text_transcription\n",
    "    X_test_speaker[transcription_id] = X_test_speaker_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uh we don't have any changes ,\n",
      "[0, 0, 0, 1]\n",
      "Can I close this ?\n",
      "do we ?\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X_train_text_before = {}\n",
    "X_train_text_after = {}\n",
    "\n",
    "list_description = [\"Parallel\", \"Correction\", \"Q-Elab\", \"Conditional\", \"Alternation\", \"Narration\", \"Background\",\"Continuation\", \"Explanation\", \"Elaboration\" , \"Acknowledgement\", \"Comment\", \"Result\", \"Question-answer_pair\", \"Contrast\", \"Clarification_question\"]\n",
    "\n",
    "X_train_description_before = {}\n",
    "X_train_description_after = {}\n",
    "for transcription_id in train_set:\n",
    "    with open(path_to_training / f\"{transcription_id}.txt\", \"r\") as file:\n",
    "        transcription = file.readlines()\n",
    "\n",
    "    X_train_text_before_transcription = [\"BEGGINING\"]\n",
    "    total_text.append(\"BEGGINING\")\n",
    "\n",
    "    X_train_text_after_transcription = [\"\"] * (len(transcription) + 1)\n",
    "    \n",
    "    vector = [0] * 16\n",
    "    X_train_description_after_transcription = [vector.copy() for i in range(len(transcription) + 1)]\n",
    "\n",
    "    description_one_hot = [0] * (len(list_description) + 1)\n",
    "    description_one_hot[0] = 1\n",
    "    X_train_description_before_transcription = [description_one_hot]\n",
    "\n",
    "    for line in transcription:\n",
    "        tab = line.split()\n",
    "\n",
    "        description_one_hot = [0] * (len(list_description) + 1)\n",
    "        description_one_hot[list_description.index(tab[1]) + 1] = 1\n",
    "        X_train_description_before_transcription.append(description_one_hot)\n",
    "\n",
    "        X_train_description_after_transcription[int(tab[0])][list_description.index(tab[1])] += 1\n",
    "        \n",
    "        X_train_text_before_transcription.append(X_train_text[transcription_id][int(tab[0])])\n",
    "\n",
    "        X_train_text_after_transcription[int(tab[0])] += X_train_text[transcription_id][int(tab[2])]\n",
    "\n",
    "    X_train_text_before[transcription_id] = X_train_text_before_transcription\n",
    "    X_train_text_after[transcription_id] = X_train_text_after_transcription\n",
    "    X_train_description_before[transcription_id] = X_train_description_before_transcription\n",
    "    X_train_description_after[transcription_id] = X_train_description_after_transcription\n",
    "\n",
    "\n",
    "X_val_text_before = {}\n",
    "X_val_text_after = {}\n",
    "X_val_description_before = {}\n",
    "X_val_description_after = {}\n",
    "for transcription_id in val_set:\n",
    "    with open(path_to_training / f\"{transcription_id}.txt\", \"r\") as file:\n",
    "        transcription = file.readlines()\n",
    "\n",
    "    X_val_text_before_transcription = [\"BEGGINING\"]\n",
    "    total_text.append(\"BEGGINING\")\n",
    "\n",
    "    X_val_text_after_transcription = [\"\"] * (len(transcription) + 1)\n",
    "\n",
    "    vector = [0] * 16\n",
    "    X_val_description_after_transcription = [vector.copy() for i in range(len(transcription) + 1)]\n",
    "\n",
    "    description_one_hot = [0] * (len(list_description) + 1)\n",
    "    description_one_hot[0] = 1\n",
    "    X_val_description_before_transcription = [description_one_hot]\n",
    "\n",
    "    for line in transcription:\n",
    "        tab = line.split()\n",
    "\n",
    "        description_one_hot = [0] * (len(list_description) + 1)\n",
    "        description_one_hot[list_description.index(tab[1]) + 1] = 1\n",
    "        X_val_description_before_transcription.append(description_one_hot)\n",
    "\n",
    "        X_val_description_after_transcription[int(tab[0])][list_description.index(tab[1])] += 1\n",
    "        \n",
    "        X_val_text_before_transcription.append(X_val_text[transcription_id][int(tab[0])])\n",
    "\n",
    "        X_val_text_after_transcription[int(tab[0])] += X_val_text[transcription_id][int(tab[2])]\n",
    "\n",
    "    X_val_text_before[transcription_id] = X_val_text_before_transcription\n",
    "    X_val_text_after[transcription_id] = X_val_text_after_transcription\n",
    "    X_val_description_before[transcription_id] = X_val_description_before_transcription\n",
    "    X_val_description_after[transcription_id] = X_val_description_after_transcription\n",
    "\n",
    "\n",
    "X_test_text_before = {}\n",
    "X_test_text_after = {}\n",
    "X_test_description_before = {}\n",
    "X_test_description_after = {}\n",
    "for transcription_id in test_set:\n",
    "    with open(path_to_test / f\"{transcription_id}.txt\", \"r\") as file:\n",
    "        transcription = file.readlines()\n",
    "\n",
    "    X_test_text_before_transcription = [\"BEGGINING\"]\n",
    "    total_text.append(\"BEGGINING\")\n",
    "\n",
    "    X_test_text_after_transcription = [\"\"] * (len(transcription) + 1)\n",
    "\n",
    "    t = [0] * 16\n",
    "    X_test_description_after_transcription = [t.copy() for i in range(len(transcription) + 1)]\n",
    "\n",
    "    description_one_hot = [0] * (len(list_description) + 1)\n",
    "    description_one_hot[0] = 1\n",
    "    X_test_description_before_transcription = [description_one_hot]\n",
    "\n",
    "    for line in transcription:\n",
    "        tab = line.split()\n",
    "\n",
    "        description_one_hot = [0] * (len(list_description) + 1)\n",
    "        description_one_hot[list_description.index(tab[1]) + 1] = 1\n",
    "        X_test_description_before_transcription.append(description_one_hot)\n",
    "\n",
    "        X_test_description_after_transcription[int(tab[0])][list_description.index(tab[1])] += 1\n",
    "\n",
    "        X_test_text_before_transcription.append(X_test_text[transcription_id][int(tab[0])])\n",
    "\n",
    "        X_test_text_after_transcription[int(tab[0])] += X_test_text[transcription_id][int(tab[2])]\n",
    "\n",
    "    X_test_text_before[transcription_id] = X_test_text_before_transcription\n",
    "    X_test_text_after[transcription_id] = X_test_text_after_transcription\n",
    "    X_test_description_before[transcription_id] = X_test_description_before_transcription\n",
    "    X_test_description_after[transcription_id] = X_test_description_after_transcription\n",
    "\n",
    "print(X_train_text[\"TS3012d\"][1])\n",
    "print(X_train_speaker[\"TS3012d\"][1])\n",
    "print(X_train_text_before[\"TS3012d\"][1])\n",
    "print(X_train_text_after[\"TS3012d\"][1])\n",
    "print(X_train_description_before[\"TS3012d\"][1])\n",
    "print(X_train_description_before[\"TS3012d\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text_tab = []\n",
    "X_train_speaker_tab = []\n",
    "X_train_text_before_tab = []\n",
    "X_train_text_after_tab = []\n",
    "X_train_description_before_tab = []\n",
    "X_train_description_after_tab = []\n",
    "y_train_tab = []\n",
    "\n",
    "for transcription_id in train_set:\n",
    "    X_train_text_tab.extend(X_train_text[transcription_id])\n",
    "    X_train_speaker_tab.extend(X_train_speaker[transcription_id])\n",
    "    X_train_text_before_tab.extend(X_train_text_before[transcription_id])\n",
    "    X_train_text_after_tab.extend(X_train_text_after[transcription_id])\n",
    "    X_train_description_before_tab.extend(X_train_description_before[transcription_id])\n",
    "    X_train_description_after_tab.extend(X_train_description_after[transcription_id])\n",
    "    y_train_tab.extend(y_train[transcription_id])\n",
    "\n",
    "X_val_text_tab = []\n",
    "X_val_speaker_tab = []\n",
    "X_val_text_before_tab = []\n",
    "X_val_text_after_tab = []\n",
    "X_val_description_before_tab = []\n",
    "X_val_description_after_tab = []\n",
    "y_val_tab = []\n",
    "\n",
    "for transcription_id in val_set:\n",
    "    X_val_text_tab.extend(X_val_text[transcription_id])\n",
    "    X_val_speaker_tab.extend(X_val_speaker[transcription_id])\n",
    "    X_val_text_before_tab.extend(X_val_text_before[transcription_id])\n",
    "    X_val_text_after_tab.extend(X_val_text_after[transcription_id])\n",
    "    X_val_description_before_tab.extend(X_val_description_before[transcription_id])\n",
    "    X_val_description_after_tab.extend(X_val_description_after[transcription_id])\n",
    "    y_val_tab.extend(y_val[transcription_id])\n",
    "\n",
    "X_test_text_tab = []\n",
    "X_test_speaker_tab = []\n",
    "X_test_text_before_tab = []\n",
    "X_test_text_after_tab = []\n",
    "X_test_description_before_tab = []\n",
    "X_test_description_after_tab = []\n",
    "\n",
    "for transcription_id in test_set:\n",
    "    X_test_text_tab.extend(X_test_text[transcription_id])\n",
    "    X_test_speaker_tab.extend(X_test_speaker[transcription_id])\n",
    "    X_test_text_before_tab.extend(X_test_text_before[transcription_id])\n",
    "    X_test_text_after_tab.extend(X_test_text_after[transcription_id])\n",
    "    X_test_description_before_tab.extend(X_test_description_before[transcription_id])\n",
    "    X_test_description_after_tab.extend(X_test_description_after[transcription_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "words=[]\n",
    "num_words = 1500\n",
    "\n",
    "total_text.append(\"First\")\n",
    "\n",
    "for text in total_text:\n",
    "    tokens=tokenizer(text)\n",
    "    words.extend(tokens)\n",
    "\n",
    "top = dict(Counter(words).most_common(1500))\n",
    "vocab = torchtext.vocab.vocab(top, specials = ['<unk>', '<pad>'])\n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=60\n",
    "\n",
    "def vectorize_sentences(reviews, max_len):\n",
    "    vectors=[]\n",
    "    for text in reviews:\n",
    "        tokens=tokenizer(text)\n",
    "        v=vocab.forward(tokens)\n",
    "        if len(v) > max_len : v = v[:max_len]\n",
    "        if len(v) < max_len : #padding\n",
    "            tmp = np.full(max_len, vocab['<pad>'])\n",
    "            tmp[0:len(v)]=v \n",
    "            v = tmp\n",
    "        vectors.append(np.array(v))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_text_vector = vectorize_sentences(X_train_text_tab, max_len)\n",
    "X_tr_speaker_vector = np.array(X_train_speaker_tab)\n",
    "X_tr_text_before_vector = vectorize_sentences(X_train_text_before_tab, max_len)\n",
    "X_tr_text_after_vector = vectorize_sentences(X_train_text_after_tab, max_len)\n",
    "X_tr_description_before_vector = np.array(X_train_description_before_tab)\n",
    "X_tr_description_after_vector = np.array(X_train_description_after_tab)\n",
    "y_tr_vector = np.array(y_train_tab).reshape(-1,1)\n",
    "\n",
    "X_va_text_vector = vectorize_sentences(X_val_text_tab, max_len)\n",
    "X_va_speaker_vector = np.array(X_val_speaker_tab)\n",
    "X_va_text_before_vector = vectorize_sentences(X_val_text_before_tab, max_len)\n",
    "X_va_text_after_vector = vectorize_sentences(X_val_text_after_tab, max_len)\n",
    "X_va_description_before_vector = np.array(X_val_description_before_tab)\n",
    "X_va_description_after_vector = np.array(X_val_description_after_tab)\n",
    "y_va_vector = np.array(y_val_tab).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\augus\\Downloads\\INF_554_MARTINEZ_GADONNEIX_CRABEIL\\jupyter_ML_model.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/augus/Downloads/INF_554_MARTINEZ_GADONNEIX_CRABEIL/jupyter_ML_model.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_rf \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Downloads/INF_554_MARTINEZ_GADONNEIX_CRABEIL/jupyter_ML_model.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/augus/Downloads/INF_554_MARTINEZ_GADONNEIX_CRABEIL/jupyter_ML_model.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model_rf\u001b[39m.\u001b[39;49mfit(X_train_rf, y_tr_vector)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Downloads/INF_554_MARTINEZ_GADONNEIX_CRABEIL/jupyter_ML_model.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_pred_train \u001b[39m=\u001b[39m model_rf\u001b[39m.\u001b[39mpredict(X_train_rf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Downloads/INF_554_MARTINEZ_GADONNEIX_CRABEIL/jupyter_ML_model.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_pred_train_class \u001b[39m=\u001b[39m (y_pred_train\u001b[39m>\u001b[39m\u001b[39m0.3\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_rf = np.concatenate((X_tr_text_vector, X_tr_speaker_vector,\n",
    "                          X_tr_text_before_vector, X_tr_text_after_vector,\n",
    "                          X_tr_description_before_vector,\n",
    "                          X_tr_description_after_vector), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Initialisation du modèle de Random Forest avec la métrique F1-score\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_rf.fit(X_train_rf, y_tr_vector)\n",
    "\n",
    "\n",
    "y_pred_train = model_rf.predict(X_train_rf)\n",
    "y_pred_train_class = (y_pred_train>0.3).astype(int)\n",
    "f1 = f1_score(y_tr_vector, y_pred_train_class)\n",
    "\n",
    "print(f\"F1-score sur les données d'entraînement : {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score sur les données d'entraînement : 0.5421530479896238\n"
     ]
    }
   ],
   "source": [
    "X_val_rf = np.concatenate((X_va_text_vector, X_va_speaker_vector,\n",
    "                          X_va_text_before_vector, X_va_text_after_vector,\n",
    "                          X_va_description_before_vector,\n",
    "                          X_va_description_after_vector), axis=1)\n",
    "y_pred_valid = model_rf.predict(X_val_rf)\n",
    "\n",
    "y_pred_valid_class = (y_pred_valid>0.3).astype(int)\n",
    "f1 = f1_score(y_va_vector, y_pred_valid_class)\n",
    "print(f\"F1-score sur les données de validation : {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score sur les données d'entraînement : 0.5252324037184595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  \n",
    "\n",
    "svm_model.fit(X_train_rf, y_tr_vector)  \n",
    "\n",
    "# Prédiction sur les données d'entraînement pour calculer le F1-score\n",
    "y_pred_train = svm_model.predict(X_train_rf)\n",
    "f1 = f1_score(y_tr_vector, y_pred_train)\n",
    "\n",
    "print(f\"F1-score sur les données d'entraînement : {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score sur les données de validation : 0.23408879230052776\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = svm_model.predict(X_val_rf)\n",
    "\n",
    "f1 = f1_score(y_va_vector, y_pred_valid)\n",
    "print(f\"F1-score sur les données de validation : {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG-Boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score sur les données d'entraînement : 0.6453870707226089\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)  \n",
    "\n",
    "\n",
    "xgb_model.fit(X_train_rf, y_tr_vector.ravel())\n",
    "\n",
    "\n",
    "y_pred_train = xgb_model.predict(X_train_rf)\n",
    "\n",
    "y_pred_train_class = (y_pred_train>0.3).astype(int)\n",
    "f1 = f1_score(y_tr_vector, y_pred_train_class)\n",
    "\n",
    "print(f\"F1-score sur les données d'entraînement : {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score sur les données d'entraînement : 0.5620813210780495\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = xgb_model.predict(X_val_rf)\n",
    "\n",
    "y_pred_valid_class = (y_pred_valid>0.3).astype(int)\n",
    "f1 = f1_score(y_va_vector, y_pred_valid_class)\n",
    "print(f\"F1-score sur les données d'entraînement : {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8ebb5d28ea55755674f1a738a1deff2071a53c72288e971b28f2d2652420cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
